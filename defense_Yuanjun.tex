\documentclass[16pt,presentation]{beamer}
%\mode<presentation>{\usetheme{default}}
%\setbeamersize{text margin left=0.5cm} 
%\setbeamersize{text margin right=0.5cm} 

\usepackage{color}
\usepackage{rotating}
\usepackage{graphicx}
\usepackage[english]{babel}
\usepackage[latin1]{inputenc}
\usepackage{helvet}
\usepackage[T1]{fontenc}
\usepackage{units}
\usepackage{bm}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{amsmath}
\usepackage{algorithm}
\usepackage{algorithmicx}
\usepackage{rotating}
\usepackage{hyperref}
\usepackage{tikz}
\usetikzlibrary{fit,positioning}
\usepackage[backend=bibtex,style=authoryear]{biblatex}
\usepackage{pifont}% http://ctan.org/pkg/pifont
\newcommand{\cmark}{\ding{51}}%
\newcommand{\xmark}{\ding{55}}%
\addbibresource{refs.bib}

\input{texdefs.tex}


%\usetheme{Warsaw}
\usetheme{metropolis}

\setbeamerfont{footline}{size=\fontsize{6}{10}\selectfont}

%%%% title etc.
\author[Yuanjun Gao]{Yuanjun Gao}
\institute{\small Department of Statistics\\ Columbia University}
\title[Department of Statistics, Columbia University]{\large Statistical Machine Learning Methods for High-dimensional Neural Population Data Analysis}
\date{}

\begin{document}

\beamertemplatetransparentcovereddynamicmedium
\setbeamertemplate{section in toc}[ball unnumbered]
\setbeamertemplate{subsection in toc}[ball unnumbered]

\begin{frame}\vspace*{1.5cm}
\maketitle
\begin{center}
\vspace{-1cm}
%Joint work with Buesing L, Shenoy KV, Cunningham JP 
%\hspace*{1.25cm}\includegraphics[width=0.44\linewidth]{./figures/CoverSlide/officialblack2.jpg} \hspace*{1cm}
\end{center}
\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%% Overview %%%%%%%%%%%%%%%%%%
\begin{frame}
\frametitle{Overview}
TODO: add a diagram for statistical criticizing

TODO: add a page for spike train
\end{frame}


\begin{frame}
\frametitle{Table of Contents}
\tableofcontents
\end{frame}


\AtBeginSection[]
{
  \begin{frame}
    \frametitle{Table of Contents}
    \tableofcontents[
    currentsection, currentsubsection
]
  \end{frame}
}

\AtBeginSubsection[]
{
  \begin{frame}
    \frametitle{Table of Contents}
    \tableofcontents[
    currentsection, currentsubsection
]
  \end{frame}
}

%%%%%%%%%%%%%%%%%%%%%%% latent variable model for neural population data %%%%%%%%
\section[]{Neural Population Data Analysis with Latent Variable Models}

\begin{frame}
\frametitle{State space models}
\begin{figure}
\centering
{\small
\begin{tikzpicture}
\tikzstyle{main}=[circle, minimum size = 11mm, thick, draw =black!80, node distance = 6mm]
\tikzstyle{connect}=[-latex, thick]
\tikzstyle{box}=[rectangle, draw=black!100]
  \node[main, fill = white!100] (z1) [] { $\vz_{t-1}$};
  \node[main] (z2) [right=of z1] {$\vz_{t}$ };
  \node[main] (z3) [right=of z2] {$\vz_{t+1}$};
  \node[main, fill = black!10] (x1) [above=of z1] { $\vx_{t-1}$};
  \node[main, fill = black!10] (x2) [above=of z2] {$\vx_{t}$ };
  \node[main, fill = black!10] (x3) [above=of z3] {$\vx_{t+1}$};
  \node (z0) [left=of z1] {$\cdots$};
  \node (zT) [right=of z3] {$\cdots$};
  \path (z1) edge [connect] (z2)
        (z2) edge [connect] (z3)
        (z1) edge [connect] (x1)
        (z2) edge [connect] (x2)
        (z3) edge [connect] (x3)
        (z0) edge [connect] (z1)
        (z3) edge [connect] (zT);
\end{tikzpicture}
}
\end{figure}
\begin{itemize}
\item $\vx_t \in \mathbb{N}^n$: spike counts; $\vz_t \in \mathbb{R}^m$: latent variables
%\item $p(\vx, \vz) = p(\vz_1) \prod_{t=1}^{T-1}p(\vz_{t+1} | \vz_t) \prod_{t=1}^T p(\vx_t | \vz_t)$
%\item Joint distribution
%\[\log p(\vx, \vz) = \underbrace{\log p(\vz_1)}_{\text{Initial distribution}} + 
%\underbrace{\sum_{t=1}^{T-1}\log p(\vz_{t+1} | \vz_t)}_{\text{Transition model}} + 
%\underbrace{\sum_{t=1}^T \log p(\vx_t | \vz_t)}_{\text{Observation model}}\]
\item Joint distribution
\[p(\vx, \vz) = \underbrace{p(\vz_1)}_{\text{Initial distribution}} 
\underbrace{\prod_{t=1}^{T-1}p(\vz_{t+1} | \vz_t)}_{\text{Transition model}} 
\underbrace{\prod_{t=1}^T p(\vx_t | \vz_t)}_{\text{Observation model}}\]
\item Common input; Dynamical view of motor data (TODO: elaborate this line)
\end{itemize}
\end{frame}


\begin{frame}
\frametitle{State space models: multiple trials}
\begin{figure}
\centering
{\footnotesize
\begin{tikzpicture}
\tikzstyle{main}=[circle, minimum size = 11mm, text width = 8mm, align=center, thick, draw =black!80, node distance = 6mm]
\tikzstyle{connect}=[-latex, thick]
\tikzstyle{box}=[rectangle, draw=black!100]
  \node[main, fill = white!100] (z1) [] { $\vz_{r(t-1)}$};
  \node[main] (z2) [right=of z1] {$\vz_{rt}$ };
  \node[main] (z3) [right=of z2] {$\vz_{r(t+1)}$};
  \node[main, fill = black!10] (x1) [above=of z1] { $\vx_{r(t-1)}$};
  \node[main, fill = black!10] (x2) [above=of z2] {$\vx_{rt}$ };
  \node[main, fill = black!10] (x3) [above=of z3] {$\vx_{r(t+1)}$};
  \node (z0) [left=of z1] {$\cdots$};
  \node (zT) [right=of z3] {$\cdots$};
  \path (z1) edge [connect] (z2)
        (z2) edge [connect] (z3)
        (z1) edge [connect] (x1)
        (z2) edge [connect] (x2)
        (z3) edge [connect] (x3)
        (z0) edge [connect] (z1)
        (z3) edge [connect] (zT);
\end{tikzpicture}
}
\end{figure}
\begin{itemize}
\item $r=1,...,R$: trial number
\item $\vx_{rt} \in \mathbb{N}^n$: spike counts; $\vz_{rt} \in \mathbb{R}^m$: latent variables
%\item $p(\vx, \vz) = p(\vz_1) \prod_{t=1}^{T-1}p(\vz_{t+1} | \vz_t) \prod_{t=1}^T p(\vx_t | \vz_t)$
%\item Joint distribution
%\[\log p(\vx, \vz) = \underbrace{\log p(\vz_1)}_{\text{Initial distribution}} + 
%\underbrace{\sum_{t=1}^{T-1}\log p(\vz_{t+1} | \vz_t)}_{\text{Transition model}} + 
%\underbrace{\sum_{t=1}^T \log p(\vx_t | \vz_t)}_{\text{Observation model}}\]
\item Joint distribution
\[p(\vx, \vz) = \prod_{r=1}^R \left[ \underbrace{p(\vz_{r1})}_{\text{Initial distribution}} 
\underbrace{\prod_{t=1}^{T-1}p(\vz_{r(t+1)} | \vz_{rt})}_{\text{Transition model}} 
\underbrace{\prod_{t=1}^T p(\vx_{rt} | \vz_{rt})}_{\text{Observation model}} \right]\]
%\item Common input; Dynamical view of motor data (TODO: elaborate this line)
\end{itemize}
\end{frame}



\begin{frame}
\frametitle{Common parameterization and our extensions}
\begin{itemize}
\item Common assumptions for latent dynamics: linear Gaussian dynamical system (LDS)
 \[\begin{split}
 \vz_{1} &\sim \N(\mu_1, Q_1)\\
 \vz_{t+1} | \vz_{t} &\sim \N(A \vz_{t}, Q)
  \end{split}\]
\item Common observation models:
 \[\vx_t | \vz_t \sim \underbrace{\underbrace{\N(C \vz_t + d, \Sigma)}_{\text{model mismatch}}
 \text{ or }
 \underbrace{\text{Poisson}\left(\exp(C \vz_t + d)\right)}_{\text{equal dispersion}}}_\text{stringent assumptions}\]
\item Our extensions for observation model:
\begin{itemize}
\item Generalized count distribution (GCLDS) \parencite{Gao2015}
\item Flexible nonlinear observation (fLDS) \parencite{gao2016linear}
\end{itemize}
\end{itemize}
\end{frame}


%%%%%%%%%%%%%%%%%%%%%%% GCLDS %%%%%%%%
\subsection[]{Generalized count linear dynamical system}
\begin{frame}
\frametitle{Motivation}
\begin{itemize}
\item Doubly stochastic Poisson model implies \alert{overdispersion}
%\[ \vx \sim \text{Poisson}(f(\vz))  , \vz \sim p(\vz) \rightarrow \text{var}(\vx) \geq E(\vx)\]
\[\left. \begin{array}{ll} \vz &\sim p(\vz) \\ \vx &\sim \text{Poisson}(f(\vz)) \end{array} \right\} \Rightarrow \alert{\text{var}(\vx) \geq E(\vx)}\]
\item Need a more flexible distribution to separate \alert{firing rate variability} with \alert{noise variability}.
\[\text{var}(\vx) = \underbrace{\text{var}\left(E(\vx | \vz)\right)}_\text{firing rate variability} + \underbrace{E\left(\text{var}(\vx | \vz)\right)}_\text{noise variability}\]
\end{itemize}
\end{frame}


\begin{frame}
\frametitle{Generalized count distribution family}
\begin{itemize}
\item Generalized count (GC) distribution family
\[\begin{split}
p_{\text{Poisson}}(x; \lambda) \propto& \frac{\exp\left\{\log{\lambda} \cdot x\right\}}{x!},~~~x \in \mathbb{N}\\
\Downarrow&\\
p_{\mathcal{GC}}(x; \theta, g(\cdot)) \propto& \frac{\exp(\theta \cdot x + g(x) )}{x!}, ~~~x \in \mathbb{N}
\end{split}\]
where $\theta \in \mathbb{R}$, $g(\cdot): \mathbb{N} \rightarrow \mathbb{R}$.
\item Parameterizes \alert{all} the count distributions \alert{redundantly}.
\item Given $g(\cdot)$, $\theta$ controls the expectation.
\item $g(\cdot)$ controls the ``shape'' of the distribution. Convex/concave $g(\cdot)$ implies over/under-dispersion.
\end{itemize}
\end{frame}


\begin{frame}
\frametitle{Model formulation}
 \begin{itemize}
 \item Linear dynamical systems with generalized count observation 
 \[\begin{split}
 \vz_{r1} &\sim \N(\mu_1, Q_1)\\
 \vz_{r(t+1)} | \vz_{rt} &\sim \N(A \vz_{rt}, Q)\\
 x_{rti} & \sim \mathcal{GC}(c_i^T \vz_{rt}, g_i(\cdot)), i = 1,...,n
 \end{split}\]
 \item Practical considerations
 \begin{itemize}
 %\item Set $g_i(0) = 0$ without loss of generality;
 \item Set $g_i(k) = -\infty$ for $k > K$ to facilitate computation;
 \item Ridge penalty on the $2^{\text{nd}}$ difference of $g_i(\cdot)$ to avoid overfitting;
 \item Set $g_i(0) = 0$ without loss of generality.
 \end{itemize}
 \end{itemize}
\end{frame}


\begin{frame}
\frametitle{Variational Bayes Expectation Maximization (VBEM)}
\begin{itemize}
\item $\vx$: data, $\vz$: latent variables, $\theta$: model parameters, 
\item Often hard to compute $p_\theta(\vx) = \int p_\theta(\vx, \vz) d \vz$ and $p_\theta(\vz | \vx)$.
\item Approximate the posterior by a \alert{tractable} distribution family.
\[p_{\theta}(\vz | \vx) \approx q(\vz) \in \mathcal{Q}\]
\item Optimize a \alert{lower bound of log likelihood}, or ELBO %w.r.t. both the model parameters and the variational distribution.
\[\begin{split}
&\text{ELBO}(\theta, q) = \int \left[\log p_{\theta}(\vx, \vz) - \log q(\vz)\right] q(\vz) d\vz \\
&= \log p_{\theta}(\vx) - \text{KL}(q(\vz) || p_\theta(\vz | \vx)) \leq \log p_{\theta}(\vx) % \log p_\theta(\vx) =& \log \int p_{\theta}(\vx, \vz) d \vz\\ 
\end{split}\]
%\[\begin{split}
%\log p_\theta(\vx) =& \log \int p_{\theta}(\vx, \vz) d \vz\\
%\geq& \int \left[\log p_{\theta}(\vx, \vz) - \log q(\vz)\right] q(\vz) d\vz
%\end{split}\]
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Variational Bayes Expectation Maximization (VBEM)}
\begin{itemize}
\item VBEM: Optimize $\text{ELBO}(\theta, q) \leq \log p_\theta(\vx)$ iteratively %by \alert{coordinate ascent}
\begin{itemize}
\item E-step: For a fixed $\theta$, optimize $q$ %(such that $q(\vz) \approx p_{\theta}(\vz | \vx)$)
\item M-step: For a fixed $q$, optimize $\theta$
%\item Iterate between E-step and M-step until convergence %, each iteration gives a higher ELBO and hopefully a higher log likelihood
\end{itemize}
\item VBEM for GCLDS 
\begin{itemize}
\item We set $q$ to be multivariate Gaussian
\item We derive a looser but tractable ELBO%The vanilla ELBO is still intractable, we use Jensen's inequality to get a tractable but looser bound. %(looser bound $\Rightarrow$ worse performance?)
%\item Fast initialization by Laplace approximation in .
\item E-step: fast Laplace approximation initialization + dual optimization
\item M-step: convex optimization + analytical solution
\end{itemize}
\end{itemize}
\end{frame}


\begin{frame}
\frametitle{Experiments}
\begin{itemize}
\item For both simulated and real dataset, we compare GCLDS with PLDS (Poisson observation model)
%\XSolid \Checkmark 
%\item On both simulated and real data
%GCLDS captures both mean and variance of the data well, PLDS capture 
\begin{center}
\begin{tabular}{ cccc } 
 \hline
  & Mean & Variance & \alert{Likelihood} \\
 \hline
 PLDS & \cmark & \xmark& \xmark \\ 
 \alert{GCLDS} &\cmark &\cmark &\cmark \\ 
 \hline
\end{tabular}
\end{center}
\end{itemize}
\end{frame}


\begin{frame}
\frametitle{Real data analysis: data}
\begin{tabular}{ cc } 
{\small Data} & {\small Variance and mean of spike counts}\\
\includegraphics[width = 0.40\textwidth]{./figs/gclds/fig_monkey.png}&
\includegraphics[width = 0.45\textwidth]{./figs/gclds/fig_var_obs_Move_seq14.pdf}
\end{tabular}
\begin{itemize}
\item Center-out reaching experiments
\item Multi-electrode array recording
\item Strong \alert{under-dispersion}
\end{itemize}
\end{frame}


\begin{frame}
\frametitle{Real data analysis: algorithms}
\begin{itemize}
\item Main algorithms to be compared
\begin{itemize}
\item \alert{PLDS}: Poisson observation
\item \alert{GCLDS-full}: Generalized count observation, individual $g(\cdot)$ across neurons
\end{itemize}
\item Two control cases for GCLDS
\begin{itemize}
\item \alert{GCLDS-linear}: truncated linear $g(\cdot)$ (truncated Poisson)
\item \alert{GCLDS-simple}: $g(\cdot)$ shared across neurons (up to a linear function)
\end{itemize}
\end{itemize}
\end{frame}


\begin{frame}
\frametitle{Real data analysis: single neuron fit}
\begin{tabular}[t]{ccc}
{\small Fitted $g(\cdot)$} & {\small Fitted mean} & {\small Fitted variance}\\
		\raisebox{-0.85\totalheight}{\includegraphics[scale=0.5,clip = true]{figs/gclds/fig_962346_seq14_xDim8_neu1_g.pdf}}&
		\raisebox{-0.85\totalheight}{\includegraphics[scale=0.5,clip = true]{figs/gclds/fig_962346_seq14_xDim8_neu1_mean.pdf}}&
		\raisebox{-0.85\totalheight}{\includegraphics[scale=0.5,clip = true]{figs/gclds/fig_962346_seq14_xDim8_neu1_var.pdf}}\\

		%B)&\raisebox{-0.85\totalheight}{\includegraphics[width=0.3\textwidth,clip = true]{figs/fig_gclds/fig_1625526_seq14_xDim8_neu2_g.pdf}}&
		%\raisebox{-0.85\totalheight}{\includegraphics[width=0.3\textwidth,clip = true]{figs/fig_gclds/fig_1625526_seq14_xDim8_neu2_mean.pdf}}&
		%\raisebox{-0.85\totalheight}{\includegraphics[width=0.3\textwidth,clip = true]{figs/fig_gclds/fig_1625526_seq14_xDim8_neu2_var.pdf}}\\
		\raisebox{-0.85\totalheight}{\includegraphics[scale=0.5,clip = true]{figs/gclds/fig_962346_seq14_xDim8_neu3_g.pdf}}&
		\raisebox{-0.85\totalheight}{\includegraphics[scale=0.5,clip = true]{figs/gclds/fig_962346_seq14_xDim8_neu3_mean.pdf}}&
		\raisebox{-0.85\totalheight}{\includegraphics[scale=0.5,clip = true]{figs/gclds/fig_962346_seq14_xDim8_neu3_var.pdf}}\\
				%&{\graphFont Dates}&\\
\end{tabular}
\end{frame}

\begin{frame}
\frametitle{Real data analysis: population fit}
%\begin{centering}
\begin{itemize}
\item Leave-one-neuron-out prediction
%\begin{itemize}
%\item Separate trials into training and testing
%\item Use training data to learn parameters
%\item For test data, drop one neuron and use other neurons to predict its firing rate
%\end{itemize}
\end{itemize}
\begin{center}
\begin{tabular}[t]{cc}
{\small MSE reduction} & {\small NLL reduction} \\
\includegraphics[scale=0.5,clip = true]{figs/gclds/fig_MSE_band_George_Move_NULL.pdf}&
\includegraphics[scale=0.5,clip = true]{figs/gclds/fig_likelihood_band_George_Move_NULL.pdf}%&
%\includegraphics[scale=0.43,clip = true]{figs/gclds/fig_var_JOB962346_seq14_xDim8_plot.pdf}
\end{tabular}
\end{center}
%\end{centering}
\end{frame}



\begin{frame}
\frametitle{Conclusion and discussion}
\begin{itemize}
\item Summary
\begin{itemize}
\item Incorporated generalized count family into state space models.
\item Developed VBEM algorithm.
\item Observed superior fitted result on real neural data.
\end{itemize}
\item Extensions
\begin{itemize}
\item $g(\cdot)$ vary across time?
\item Share information of $g(\cdot)$ across neurons? (hierarchical model?)
\item Generative models for under-dispersion?
\end{itemize}
\end{itemize}
\end{frame}



%%%%%%%%%%%%%%%%%%%%%%% fLDS %%%%%%%%
\subsection[]{Linear dynamical neural population models through nonlinear embeddings}

\begin{frame}
\frametitle{Motivation}
\begin{tabular}{cl}
\parbox{0.6\textwidth}{
\begin{itemize}
\item Neural activities lie in a low-dimensional \alert{nonlinear manifold} rather than a \alert{linear subspace}
\item Flexible observation model makes the state space model more expressive
\end{itemize}
}
&
\hspace{-1cm}
\begin{tabular}{c}
\includegraphics[scale=0.5,clip = true]{figs/flds/fig_V1_single_3concat_onlyobservation.pdf}
\end{tabular}
\end{tabular}
\end{frame}


\begin{frame}
\frametitle{Model formulation: fLDS}
\begin{itemize}
 \item Linear dynamical systems with \alert{nonlinear link} and count observation
 \[\begin{split}
 \vz_{r1} \sim& \N(\mu_1, Q_1)\\
 \vz_{r(t+1)} | \vz_{rt} \sim& \N(A \vz_{rt}, Q)\\
 x_{rti} \sim& \text{Poisson}(\alert{f}_i(\vz_{rt})) \text{ (PfLDS) } \\
 &\text{   or }\mathcal{GC}(\alert{f}_i(\vz_{rt}), g_i(\cdot)) \text{ (GCfLDS)}%, i = 1,...,n
 \end{split}\]
 where $f_i$ is a nonlinear function parameterized by a neural network
 \item Linear dynamics: simple, tractable, interpretable
 \item Nonlinear observation: flexibility
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Inference algorithm: AEVB (high level idea)}
%\begin{itemize}
%\item Auto-encoding Variational Bayes (AEVB)
\begin{itemize}
\item Auto-encoding Variational Bayes (AEVB)
\item Learn a mapping (recognition model) from data to the \alert{approximate posterior distribution of latent variable}.
\item Jointly optimize the generative model parameters and recognition model parameters.
\item Naturally incorporate stochastic optimization to handle large datasets.
\end{itemize}
%\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Inference algorithm: AEVB (algorithm)}
\begin{itemize}
\item Decompose ELBO by trials
\[\begin{split}
\text{ELBO}(\theta, q) %=& \int \left[\log p_{\theta}(\vx, \vz) - \log q(\vz)\right] q(\vz) d\vz\\
=& \sum_{r=1}^{R} \int \left[\log p_{\theta}(\vx_r, \vz_r) - \log q(\vz_r)\right] q(\vz_r) d\vz_r 
\end{split}\]
\item Map data $\vx_r$ to $q(\vz_r)$ by a parameterized function
\[q(\vz_r) = q_\phi(\vz_r; \vx_r) = \N\left(\mu_\phi(\vx_r), \Sigma_\phi(\vx_r)\right)\]
\item Learn both $\theta$ and $\phi$ by optimizing ELBO
\[\text{ELBO}(\theta, \phi) = \sum_{r=1}^{R} \int \left[\log p_{\theta}(\vx_r, \vz_r) - \log q_\phi(\vz_r; \vx_r)\right] q_\phi(\vz_r; \vx_r) d\vz_r \]
\item Do stochastic optimization with gradient of a single trial
\end{itemize}
\end{frame}


\begin{frame}
\frametitle{Inference algorithm: AEVB (important details)}
\begin{itemize}
\item Specific parameterization of the recognition model
\[q(\vz_r) = q_\phi(\vz_r; \vx_r) = \N\left(\mu_\phi(\vx_r), \Sigma_\phi(\vx_r)\right)\]
\vspace{-0.5cm}
\begin{itemize}
\item Block tri-diagonal precision matrix that agrees with Markovian structure
\item Potentially useful to perform filtering in an online fashion
\end{itemize}
\item Reparameterization trick for stochastic optimization
\begin{itemize}
\item Easy implementation
\item Low variance
\end{itemize}
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Experiments}
\begin{center}
\begin{tabular}{ ccccc } 
 \hline
  & Mean & Variance & Likelihood & \alert{Concise representation}\\
 \hline
 PLDS & \cmark & \xmark& \xmark &  \xmark  \\ 
 GCLDS &\cmark &\cmark &\cmark &  \xmark  \\ 
 \alert{PfLDS} & \cmark & \xmark& \xmark &   \cmark\\ 
 \alert{GCfLDS} &\cmark &\cmark &\cmark &  \cmark \\ 
 \hline
\end{tabular}
\end{center}
\end{frame}

\begin{frame}
\frametitle{Real data analysis: primate visual cortex}
%\hspace{-1cm}
\begin{tabular}{cc}
\small{Firing rate} & \\
\begin{tabular}{c}
\includegraphics[scale=0.45,clip = true]{figs/flds/fig_V1_single_3concat_onlyobservation.pdf} 
\end{tabular}
&
\begin{tabular}{c}
%PLDS \\
%\includegraphics[scale=0.5,clip = true]{figs/flds/fig_V1_mat_trajectory_3concat.pdf}\\
%PfLDS \\
%\includegraphics[scale=0.5,clip = true]{figs/flds/fig_V1_py_trajectory_3concat.pdf}
\end{tabular}
\end{tabular}
\end{frame}



\begin{frame}
\frametitle{Real data analysis: primate visual cortex}
%\hspace{-1.2cm}
\begin{tabular}{ccc}
{\small \visible<1->Firing rate} & \visible<2->{\hspace{-0.6cm}\small{Latent projection}} & \visible<3->{\hspace{-0.6cm}\small{1-step-ahead prediction}}\\
%\hline
\begin{tabular}{c}
\visible<1->{\includegraphics[scale=0.45,clip = true]{figs/flds/fig_V1_single_3concat.pdf}}
%\visible<2->{\includegraphics[scale=0.5,clip = true]{figs/flds/fig_V1_single_3concat.pdf} }
\end{tabular}
&
\hspace{-0.6cm}
\begin{tabular}{c}
\visible<2->{\scriptsize{PLDS} \\
\includegraphics[scale=0.4,clip = true]{figs/flds/fig_V1_mat_trajectory_3concat.pdf}\\
\scriptsize{PfLDS} \\
\includegraphics[scale=0.4,clip = true]{figs/flds/fig_V1_py_trajectory_3concat.pdf}}
\end{tabular}
&
\hspace{-0.6cm}
\visible<3->{\begin{tabular}{c}
\scriptsize{MSE reduction}\\
\includegraphics[scale=0.40,clip = true, trim = 0cm 0cm 0cm 0.2cm]{figs/flds/fig_V1_MSE_3concat.pdf}\\
\scriptsize{NLL reduction}\\
\includegraphics[scale=0.40,clip = true, trim = 0cm 0cm 0cm 0.2cm]{figs/flds/fig_V1_NLL_3concat.pdf}\\
\end{tabular}}
\end{tabular}
\end{frame}


\begin{frame}
\frametitle{Real data analysis: Primate motor cortex}
\begin{tabular}{ cc } 
{\small Data} & {\small Reaching trajectory} \\
\includegraphics[width = 0.45\textwidth]{./figs/gclds/fig_monkey.png}&
\includegraphics[width = 0.40\textwidth]{./figs/flds/fig_GeorgeMove_trajectory.pdf}
\end{tabular}
\end{frame}


\begin{frame}
\frametitle{Real data analysis: Primate motor cortex}
\begin{itemize}
\item Latent projection with $2$ latent dimensions
\end{itemize}
\begin{tabular}{ ccc } 
{\small Reaching trajectory} & {\small PLDS} & {\small PfLDS} \\
\includegraphics[width = 0.30\textwidth]{./figs/flds/fig_GeorgeMove_trajectory.pdf}&
\includegraphics[width = 0.30\textwidth]{./figs/flds/fig_GeorgeMove_2dvisual_mat_PLDS.pdf}&
\includegraphics[width = 0.30\textwidth]{./figs/flds/fig_GeorgeMove_2dvisual_py_PLDS.pdf}
\end{tabular}
\end{frame}

\begin{frame}
\frametitle{Real data analysis: Primate motor cortex}
\begin{itemize}
\item One-step-ahead predictive performance
\end{itemize}
\begin{center}
\begin{tabular}{ cc} 
{\small MSE reduction} & {\small NLL reduction}\\
\includegraphics[scale = 0.6]{./figs/flds/fig_GeorgeMove_MSE_GC.pdf}&
\includegraphics[scale = 0.6]{./figs/flds/fig_GeorgeMove_NLL_GC.pdf}
\end{tabular}
\end{center}
\end{frame}


\begin{frame}
\frametitle{Conclusion and discussion}
\begin{itemize}
\item Summary
\begin{itemize}
\item Incorporated nonlinear observation into state space models.
\item Developed AEVB algorithm (flexible and scalable).
\item Obtain concise latent representation.
\end{itemize}
\item Future work
\begin{itemize}
\item Better stochastic optimization scheme
\item Interpretable nonlinearity
\item Application on more complex datasets
\end{itemize}
\end{itemize}
\end{frame}


%%%%%%%%%%%%%%%%%%%%%% ROI detection %%%%%%%%%%%%%%%%%%%%%%%%%
\section[]{Region of Interest Detection for Calcium Imaging Data}

\begin{frame}
\frametitle{Introduction: calcium imaging data}
TODO: incorporate a video?
\end{frame}

\begin{frame}
\frametitle{Model formulation: idea}
\begin{itemize}
\item  $X \in \mathbb{R}^{N \times T}$ represents the calcium imaging data, where each column is a (vectorized) frame that contains $N$ pixels
\item Decompose $X$ into a product of $K$ \alert{spatial component} and \alert{temporal component}% (neural acitivities)
\[X = D \cdot A^T + \text{noise}\]
\begin{itemize}
\item $D = [D_1,...,D_K] \in \mathbb{R}^{N \times K}$ represents the neuron shapes
\item $A = [A_1,...,A_K] \in \mathbb{R}^{T \times K}$ is the neural activities%, $K$ is the number of neurons.
\end{itemize}
\item Further exploit structure of the components (localized neuron shapes)
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Model formulation: objective}
\begin{itemize}
\item Structured matrix factorization
\begin{equation}
%\label{equ:opt}
\begin{aligned}
& \underset{D, A}{\text{minimize}}
& & \| X - D A^T \|_2^2 + f_D(D), \\
& \text{subject to}
& & D_k \in \mathcal{D}_{w}^+; k = 1, \ldots, K,\\
& 
& & \|A_k\|_2 \leq c_k,
\end{aligned}
\end{equation}
\item $\mathcal{D}_{w}^+$: non-negative vectors whose nonzero values is within a $w \times w$ window
\item $f_D(D)$ regularizes the neuron shape (discussed later)
\item $\|A_k\|_2 \leq c_k$ avoids degenerate solution
\end{itemize}
\end{frame}


\begin{frame}
\frametitle{Greedy algorithm}
\begin{itemize}
\item Scan the each frame of the video with a small Gaussian kernel
\item At iteration $k$, given the current residue (unexplained by existing ROI)
\begin{itemize}
\item \alert{Greedy identification}: Identify the location $p_k$ where the Gaussian kernel explains most of the data (across time)
\item \alert{Shape fine tuning}: Locally optimize the spatial and temporal component
\item \alert{Residue update}: Subtract the newly identified ROI
\end{itemize}
\end{itemize}
\end{frame}


\begin{frame}
\frametitle{Shape fine tuning}
\begin{itemize}
\item Given current residue $R$, an identified center pixel $p_k$, denote $S_k$ as a $w \times w$ window centered at $p_k$
\begin{equation}
\begin{aligned}
& \underset{D_k, A_k}{\text{minimize}}
& & \| R - D_k A_k^T \|^2 + f(D_k), \\%\sum_{i = 1}^3 \lambda_i f_i(D_k),\\
& \text{subject to}
& & D_{kp} \geq 0, p \in S_k,\\
&
& & D_{kp} = 0, p \notin S_k,\\
& 
& & \|A_k\|_2 \leq c_k,
\end{aligned}
\end{equation}
%\begin{itemize}
\item $f(D_k) = \sum_{i=1}^3 \lambda_i f_i(D_k)$
%where
\begin{itemize}
\item $f_1(D_k) = \sum_p \tau_{(p, p_k)} | D_{kp} |$ encourages sparsity
\item $f_2(D_k) = \sum_p (D_{kp} - G_{p_k})^2$ encourage Gaussian shape
\item $f_3(D_k) = \sum_{\text{$p_1$ and $p_2$ are neighbors}} (D_{kp_1} - D_{kp_2})^2$ encourages smoothness
\end{itemize}
\item Optimize $D_k$ and $A_k$ by block coordinate descent
%\item Need to tune regularization parameters, not necessarily good since those would shrink the estimated shape
\end{itemize}
%\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Real data analysis: sample patch, no shape regularization}
\begin{tabular}[t]{cc}
%\multicolumn{2}{c}{Gaussian kernel shape}  \\
\includegraphics[scale=0.35,clip = true]{figs/ROI/fig_Misha_plain_comp.pdf}&
\includegraphics[scale=0.35,clip = true]{figs/ROI/fig_Misha_plain_shape.pdf}\\
%\multicolumn{2}{c}{Ring shape} \\
%\includegraphics[scale=0.45,clip = true]{figs/fig_ROI/fig_simulation_comp_Ring.pdf}&
%\includegraphics[scale=0.45,clip = true]{figs/fig_ROI/fig_simulation_shape_Ring.pdf}\\
%\\
\end{tabular}
\end{frame}

\begin{frame}
\frametitle{Real data analysis: sample patch, shape regularization}
\begin{tabular}[t]{cc}
%\multicolumn{2}{c}{Gaussian kernel shape}  \\
\includegraphics[scale=0.35,clip = true]{figs/ROI/fig_Misha_smooth_comp.pdf}&
\includegraphics[scale=0.35,clip = true]{figs/ROI/fig_Misha_smooth_shape.pdf}\\
%\multicolumn{2}{c}{Ring shape} \\
%\includegraphics[scale=0.45,clip = true]{figs/fig_ROI/fig_simulation_comp_Ring.pdf}&
%\includegraphics[scale=0.45,clip = true]{figs/fig_ROI/fig_simulation_shape_Ring.pdf}\\
%\\
\end{tabular}
\end{frame}


\begin{frame}
\frametitle{Conclusion and discussion}
\begin{itemize}
\item Summary
\begin{itemize}
\item Formulating calcium imaging ROI detection as a structure matrix factorization problem
\item Greedy algorithm with shape regularization
\item Fast ROI detection algorithm
\end{itemize}
\item Future work
\begin{itemize}
\item More spatial and temporal structure
\item Overlapping neuron
\item Online ROI detection
\item Motion correction, background elimination
\end{itemize}
\end{itemize}
\end{frame}


%%%%%%%%%%%%%%%%%%%%% MEFN %%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section[]{Maximum Entropy Flow Networks}

\begin{frame}
\frametitle{Introduction: maximum entropy principle}
\begin{itemize}
\item \alert{Maximum entropy principle}: Subject to some given prior knowledge (moment/support constraints), the distribution that makes \alert{minimal additional assumptions} is that which has the \alert{largest entropy} of any distribution obeying those constraints
\item Useful for model formulation and hypothesis testing.
\item \alert{Problem}: hard to learn and sample from maximum entropy distribution when constraints are complicated
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Gibbs distribution}
\end{frame}

\begin{frame}
\frametitle{Normalizing flow}
\end{frame}

\begin{frame}
\frametitle{MEFN formulation}
\end{frame}

\begin{frame}
\frametitle{Learning by augmented Lagrangian method}
\end{frame}

\begin{frame}
\frametitle{Simulation: Dirichlet}
\end{frame}

\begin{frame}
\frametitle{Application: Texture modeling}
\end{frame}

\begin{frame}
\frametitle{Conclusion}
\end{frame}

\begin{frame}
\frametitle{Acknowledgement}
\end{frame}

\end{document}


